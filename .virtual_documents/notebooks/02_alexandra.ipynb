import pandas as pd 
from sklearn.datasets import fetch_california_housing
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsRegressor

from sklearn.preprocessing import MinMaxScaler, StandardScaler


df = "/Users/ZINA/Desktop/IRONHACK/Week_7/parkinsons/telemonitoring/parkinsons_updrs.data"
park_data = pd.read_csv(df)
park_data.dropna(how='all', inplace=True)
park_data.drop_duplicates(inplace=True)
park_data


park_data.isnull().sum()


park_data.columns


park_data['subject#'].nunique()


park_data['age'].min()


park_data['age'].max()





age_counts = park_data['sex'].value_counts()

plt.figure()
age_counts.plot(kind='pie', autopct='%1.1f%%')
plt.ylabel('')  # removes the y-label
plt.title('Sex Distribution')
plt.show()





plt.figure()
plt.hist(park_data['age'], bins=30)  # increase bins
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.title('Age Distribution')
plt.show()








features = park_data.drop(columns = ["total_UPDRS"])
target = park_data["total_UPDRS"]





from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.20, random_state=0)


X_train.head()


y_train.head()


from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor





normalizer = MinMaxScaler() # x_new = (x - min(x)) / (max(x) -min(x))

normalizer.fit(X_train)


scaler = StandardScaler() # x_new = (x - mean(x)) / std(x)

scaler.fit(X_train)


X_train_norm_np = normalizer.transform(X_train)
X_test_norm_np = normalizer.transform(X_test)


X_train_norm_df = pd.DataFrame(X_train_norm_np, columns = X_train.columns, index=X_train.index)
X_train_norm_df.head()


X_test_norm_df = pd.DataFrame(X_test_norm_np, columns = X_test.columns, index=X_test.index)
X_test_norm_df.head()


X_train_standarized_np = scaler.transform(X_train)
X_test_standarized_np = scaler.transform(X_test)

X_train_standarized_df = pd.DataFrame(X_train_standarized_np, columns = X_train.columns, index=X_train.index)
X_test_standarized_df  = pd.DataFrame(X_test_standarized_np, columns = X_test.columns, index=X_test.index)


X_train_norm_df.describe()


X_train_standarized_df.describe()





lin_reg = LinearRegression()


lin_reg2 = LinearRegression()


lin_reg.fit(X_train_norm_df, y_train)


lin_reg2.fit(X_train_standarized_df, y_train)


import pickle

with open("linear_model_normalized.pkl", "wb") as file:
    pickle.dump(lin_reg, file)

with open("min_max_scaler.pkl", "wb") as file:
    pickle.dump(normalizer, file)


from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, root_mean_squared_error


y_pred_test = lin_reg.predict(X_test_norm_df)

print(f"MAE {mean_absolute_error(y_pred_test, y_test): .2f}") # mean(abs(error)) = mean(abs(y_test - y_pred_test))
print(f"MSE {mean_squared_error(y_pred_test, y_test): .2f}") # mean(error**2) = mean((y_test - y_pred_test)**2)
print(f"RMSE, {root_mean_squared_error(y_pred_test, y_test): .2f}") # sqrt( mean( (y_test - y_pred_test)**2 ) ) # b0, b1, b2...
print(f"R2 score, {lin_reg.score(X_test_norm_df, y_test): .2f}") # r2_score


sns.scatterplot(x=y_test, y=y_pred_test); # y_pred_test = y_test;  y_pred_test = 0 + y_test; y_pred_test = 0 + 1 * y_test


y_pred_test2 = lin_reg2.predict(X_test_standarized_df)

#lin_reg.score(X_test_norm_df, y_test)

print(f"MAE {mean_absolute_error(y_pred_test2, y_test): .2f}") # mean(abs(error)) = mean(abs(y_test - y_pred_test))
print(f"MSE {mean_squared_error(y_pred_test2, y_test): .2f}")
print(f"RMSE, {root_mean_squared_error(y_pred_test2, y_test): .2f}") # sqrt( mean( (y_test - y_pred_test)^2 ) ) # b0, b1, b2...
print(f"R2 score, {lin_reg2.score(X_test_standarized_df, y_test): .2f}")


sns.scatterplot(x=y_test, y=y_pred_test2);


results = pd.DataFrame({"y_test": y_test, "lin_reg": y_pred_test, "lin_reg2": y_pred_test2})
results


results_melted = results.melt(id_vars="y_test")
results_melted


sns.scatterplot(data=results_melted, y="value", x="y_test", hue="variable");


lin_reg_coef = {feature : coef for feature, coef in zip(X_train_norm_df.columns, lin_reg.coef_)}
lin_reg_coef


lin_reg.intercept_








## Advanced Modeling


## Random Forest Regressor


from sklearn.ensemble import RandomForestRegressor


rf_model = RandomForestRegressor(n_estimators=100,random_state=42)

rf_model.fit(X_train, y_train)
y_pred_rf = rf_model.predict(X_test)


rf_mae = mean_absolute_error(y_test, y_pred_rf)
rf_r2 = r2_score(y_test, y_pred_rf)

print("Random Forest MAE:", rf_mae)
print("Random Forest RÂ²:", rf_r2)




